<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Automate Red Hat JBoss Web Server deployments with Ansible</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lYvQvwWxVTc/automate-red-hat-jboss-web-server-deployments-ansible" /><author><name>Romain Pelisse</name></author><id>bb17fe33-34bd-4ea4-ae15-f2317bd13289</id><updated>2021-08-30T07:00:00Z</updated><published>2021-08-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/webserver/overview"&gt;Red Hat JBoss Web Server&lt;/a&gt; combines a web server (Apache HTTPD), a servlet engine (Apache Tomcat), and modules for load balancing (&lt;code&gt;mod_jk&lt;/code&gt; and &lt;code&gt;mod_cluster&lt;/code&gt;). Ansible is one of the best automation tools on the market. In this article, we'll use Ansible to completely automate the deployment of a JBoss Web Server instance on a freshly installed Red Hat Enterprise Linux (RHEL) server.&lt;/p&gt; &lt;p&gt;Our objective is to automate a JBoss Web Server deployment through the following tasks:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Retrieve the archive containing JBoss Web Server from a repository and install the files it contains on the system.&lt;/li&gt; &lt;li&gt;Configure the Red Hat Enterprise Linux operating system (create users, groups, and the required setup files to make JBoss Web Server a systemd service).&lt;/li&gt; &lt;li&gt;Fine-tune the configuration of the JBoss Web Server server itself (bind it to the appropriate interface and port).&lt;/li&gt; &lt;li&gt;Deploy a web application and starting the systemd service.&lt;/li&gt; &lt;li&gt;Perform a health check if the deployed application is accessible.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Ansible fully automates these operations, with no manual steps required.&lt;/p&gt; &lt;h2&gt;Set the target environment&lt;/h2&gt; &lt;p&gt;Before we start the automation, we need to specify our target environment. In this case, we're using RHEL 8 with Python 3.6.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cat /etc/redhat-release Red Hat Enterprise Linux release 8.4 (Ootpa) # ansible --version ansible 2.9.22   config file = /etc/ansible/ansible.cfg   configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']   ansible python module location = /usr/lib/python3.6/site-packages/ansible   executable location = /usr/bin/ansible   python version = 3.6.8 (default, Mar 18 2021, 08:58:41) [GCC 8.4.1 20200928 (Red Hat 8.4.1-1)]&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook might not work if you want to use a different Python version or target operating system.&lt;/p&gt; &lt;h2&gt;Install the JBoss Web Server Ansible collection&lt;/h2&gt; &lt;p&gt;Once you have RHEL 8 set up and Ansible ready to go, you need to install the &lt;a href="https://galaxy.ansible.com/middleware_automation/jws"&gt;JBoss Web Server Ansible collection&lt;/a&gt;. Ansible uses the collection to perform the following tasks on JBoss Web Server:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Ensure required dependencies are installed (e.g., unzip).&lt;/li&gt; &lt;li&gt;Install Java (if missing and requested).&lt;/li&gt; &lt;li&gt;Install the binaries and integrate the software into the system (user, group, etc.).&lt;/li&gt; &lt;li&gt;Deploy the configuration files.&lt;/li&gt; &lt;li&gt;Start and enable JBoss Web Server as a systemd service.&lt;/li&gt; &lt;li&gt;Configure a Password Vault with JBoss Web Server and a load balancer (e.g., &lt;code&gt;mod_cluster&lt;/code&gt;)—not used for this article.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here's the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-galaxy collection install middleware_automation.jws Process install dependency map Starting collection install process Installing 'middleware_automation.jws:0.0.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/jws' Installing 'middleware_automation.redhat_csp_download:1.1.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download'&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://galaxy.ansible.com/"&gt;Ansible Galaxy&lt;/a&gt; fetches and downloads the collection's dependencies. That is why, at the end of the execution, the JBoss Web Server collection was installed, as well as &lt;code&gt;redhat_csp_download&lt;/code&gt;, which will help facilitate the retrieval of the archive containing the JBoss Web Server server.&lt;/p&gt; &lt;p&gt;Installing the collection reduces the configuration to achieve our automation to the bare minimum.&lt;/p&gt; &lt;h2&gt;Create a playbook to test the installation&lt;/h2&gt; &lt;p&gt;Before we continue, let's create a minimal playbook to confirm that the collection was properly installed:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   collections: – middleware_automation.jws   tasks :&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In its current state, this playbook doesn’t perform any tasks on the target system. If the playbook runs successfully, then we know that the collection has been properly installed.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts min.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the Apache Tomcat web server&lt;/h2&gt; &lt;p&gt;Next, we'll install the Apache Tomcat web server, which consists of several steps.&lt;/p&gt; &lt;h3&gt;Download the archive&lt;/h3&gt; &lt;p&gt;First, let's modify our minimal playbook to retrieve the archive containing Apache Tomcat. For this purpose, we will leverage the &lt;code&gt;get_url&lt;/code&gt; module from Ansible:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_version: 9.0.50 tomcat_download_url: https://archive.apache.org/dist/tomcat/tomcat-9/v{{tomcat_version}}/bin/apache-tomcat-{{tomcat_version}}.zip tomcat_install_dir: /opt tomcat_zipfile: “{{tomcat_install_dir}}/tomcat.zip"   collections: - middleware_automation.jws pre_tasks: - name: "Download latest JBoss Web Server Zipfile from {{ tomcat_download_url }}."    get_url:      url: "{{ tomcat_download_url }}"      dest: "{{ tomcat_zipfile }}"             remote_src: yes    when:      - tomcat_download_url is defined&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook downloads the archive during the &lt;code&gt;pre_tasks&lt;/code&gt; section. It’s important to use the role provided by the JBoss Web Server collection in the next step. This role will be executed before the &lt;code&gt;tasks&lt;/code&gt; block and after the &lt;code&gt;pre_tasks&lt;/code&gt; block, and it requires that the archive file be already present on the target system.&lt;/p&gt; &lt;p&gt;Execute a new playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts playbook.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] TASK [Download latest JBoss Web Server Zipfile from https://archive.apache.org/dist/tomcat/tomcat-9/v9.0.50/bin/apache-tomcat-9.0.50.zip.] *** changed: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java&lt;/h3&gt; &lt;p&gt;JBoss Web Server is a Java-based server, so the target system is required to install a Java Virtual Machine (JVM). While Ansible primitives can perform such tasks natively, the &lt;code&gt;jws&lt;/code&gt; role can also take care of this part, provided the &lt;code&gt;tomcat_java_version&lt;/code&gt; variable is defined:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: ... tomcat_java_version: 1.8.0   collections: – middleware_automation.jws …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind that this feature has limits; it works only if the target system’s distribution belongs to the Red Hat &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#ansible-facts-os-family"&gt;family&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible -m setup localhost | grep family      "ansible_os_family": "RedHat",&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java web server&lt;/h3&gt; &lt;p&gt;For Java web server to work, we need to provide one more variable to our playbook. The &lt;code&gt;tomcat_setup&lt;/code&gt; (set to &lt;code&gt;true&lt;/code&gt;) signals to the &lt;code&gt;jws&lt;/code&gt; role that we want it to perform the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_setup: true ...   collections: - middleware_automation.jws   roles: - jws   pre_tasks: ...   tasks:&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Run the playbook again&lt;/h2&gt; &lt;p&gt;Let’s &lt;a href="https://gist.github.com/rpelisse/7e97b53c080c5a14ea601cf6ba5cc8bc"&gt;run our playbook again&lt;/a&gt; to see if it works as expected.&lt;/p&gt; &lt;p&gt;As you can see, quite a lot happened during this execution. Indeed, the &lt;code&gt;jws&lt;/code&gt; role took care of all the setup:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Deploying a base configuration.&lt;/li&gt; &lt;li&gt;Removing unused applications.&lt;/li&gt; &lt;li&gt;Starting the web server.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The playbooks also perform a few tasks on the target system, like installing any required dependencies if they are missing and ensuring the environment is properly configured.&lt;/p&gt; &lt;h2&gt;Configure JBoss Web Server as a systemd service&lt;/h2&gt; &lt;p&gt;A nice feature of the &lt;code&gt;jws&lt;/code&gt; role is the included functionality to configure JBoss Web Server as a systemd service. For this to occur, you just need to define the &lt;code&gt;tomcat_service_name&lt;/code&gt; variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: … tomcat_service_name: tomcat ...   collections: - middleware_automation.jws   roles:      …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind this only works when systemd is installed and the system belongs to the Red Hat family.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status tomcat ● tomcat.service - JBoss Web Server Web Application Container   Loaded: loaded (/usr/lib/systemd/system/tomcat.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2021-07-22 16:52:08 UTC; 16h ago    Main PID: 6234 (java)    Tasks: 37 (limit: 307)   Memory: 187.4M      CPU: 21.528s   CGroup: /system.slice/tomcat.service └─6234 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-9.0.50/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-9.0.50/bin/bootstrap.jar:/opt/apache-tomcat-9.0.50/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-9.0.50 -Dcatalina.home=/opt/apache-tomcat-9.0.50 -Djava.io.tmpdir=/opt/apache-tomcat-9.0.50/temp org.apache.catalina.startup.Bootstrap start Jul 22 16:52:08 4414af93c931 systemd[1]: Started Apache Tomcat Web Application Container. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6220]: Tomcat started. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6219]: Tomcat runs with PID: 6234&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy a web application&lt;/h2&gt; &lt;p&gt;Now that JBoss Web Server is running, let’s modify the playbook and facilitate the deployment of a web application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;tasks:     - name: " Checks that server is running"       uri:         url: "http://localhost:8080/"         status_code: 404         return_content: no     - name: "Deploy demo webapp"       get_url:         url: 'https://people.redhat.com/~rpelisse/info-1.0.war'         dest: "{{ tomcat_home }}/webapps/info.war"       notify:         - Restart Tomcat service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_handlers.html"&gt;handler&lt;/a&gt; in the &lt;code&gt;jws&lt;/code&gt; role restarts JBoss Web Server when the web application is downloaded. To finish our demonstration, we can add a quick test in the &lt;code&gt;post_tasks&lt;/code&gt; section of the playbook to confirm that the web application is functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;  post_tasks: - name: "Sleep for {{ tomcat_sleep }} seconds to let Tomcat starts "    wait_for:      timeout: "{{ tomcat_sleep }}" - name: "Test application"    get_url:      url: "http://localhost:8080/info/"      dest: /tmp/info.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;That’s all for today! Future articles will demonstrate other features provided by the Red Hat JBoss Web Server collection, including support for &lt;code&gt;mod_cluster&lt;/code&gt; and securing the server with Tomcat’s Vault feature.&lt;/p&gt; &lt;p&gt;In the meantime, you can find the playbook used for this article in the &lt;a href="https://github.com/ansible-middleware/jws-ansible-playbook/blob/trunk/playbook.yml"&gt;Ansible Collection for JBoss Web Server GitHub repository&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible" title="Automate Red Hat JBoss Web Server deployments with Ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lYvQvwWxVTc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2021-08-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible</feedburner:origLink></entry><entry><title type="html">Emitting Process events to Kafka for Analytics using Red Hat Process Automation Manager</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FLvqxonrVD8/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/08/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html</id><updated>2021-08-27T20:22:17Z</updated><content type="html">Red Hat AMQ Streams, based on Apache Kafka, is a streaming platform. Since the 7.10 release of Red Hat Process Automation Manager, users have access to out-of-box Kafka integration. Easily integrate the business processes with Kafka, for emitting or consuming events. Earlier this month, I wrote an o.n how to create a business process using Red Hat Process Automation Manager and Red Hat AMQ Streams on OpenShift. In this article, we will see how you can configure the KIE Server to emit Kafka messages about every event when a process, case, or task is completed. The KIE Server sends the messages when it commits transactions. These events can then be pushed to an analytics engine for visualizing the process metrics. Step 1: Deploy the AMQ Streams operator The Operator Hub is a collection of Operators from the Kubernetes community and Red Hat partners, curated by Red Hat. Let us first create a install the AMQ Streams operator. We will install it in a namespace we have created (rhpam-monitoring in the example). Now that the operator is installed, we can now create a simple 3 node Kafka cluster. For this click on the Kafka tab and click on the Create Kafka button. We will accept the defaults and create. Step 2: Setup Business Automation Operator Next we will first setup the business automation operator. Search for the Business Automation Operator from the operator hub. The operator allows you to create a Red Hat Process Automation manager environment with the authoring and deployment capabilities. Once the operator is deployed, switch over to the KieApp tab and click on Create KieApp. In this wizard, click on the Objects section and open up Servers. Here we will create a Kieserver definition with the environment properties for emitting the process server events. Let us now define the Bootstrap Servers as my-cluster-kafka-brokers:9092. By default, the KIE Server publishes the messages in the following topics: * jbpm-processes-events * jbpm-tasks-events * jbpm-cases-events It is possible to modify these topic names. Let us define the Processes Topic Name as rhpam-processes and the Tasks Topic Name as rhpam-tasks. For a complete list of the properties and its configuration, check out the documentation . This configuration is enough to push Process metrics to Kafka. You do not need to change anything in the process design. The configuration can be deployed using a definition as well. Step 3: Deploy a business process and create an instance Let us now open Business Central. For this lookup the route definition from the namespace. Login in to Business Central with the username/password as defined in the Business Central configuration. Import the by clicking on the Import Project. We will build and deploy the changes. Now we will create a process instance, with the following values. You can see that the process instance is created. Step 4: Push Kafka messages to a analytics tool Now that we have configured the Kafka broker, the process metrics should have been emitted to the corresponding topics. To verify this, let us deploy an open source Kafka UI ( Kafdrop) to check out the events. For deploying this, we can use the following command. oc apply -f https://raw.githubusercontent.com/snandakumar87/transaction-monitoring-dmn-kogito/master/ocp-deploy/kafdrop.yml The yaml definition for installing Kafdrop can be found . Once the deployment is completed, we can access the route definition to open up the kafka UI. You should now see the two topics created for Process and Task events. Notice how the messages in these topics are in the format. Once the metrics data is available, it is possible to push to have multiple downstream consumers reading from these topics. Let us push these metrics to an Elastic cluster. For this let us deploy the Elastic operator on Openshift.  We will create an ElasticSearchCluster and an instance of Kibana. Let us accept the defaults to complete the setup.  Now that the elastic instance is deployed, we will use a simple class to read data from the kafka topic and push it to elastic. We have defined two routes, to push the metrics to the elastic cluster.  from("kafka:" + "rhpam-processes" + "?brokers=" + kafkaBootstrap + "&amp;amp;maxPollRecords=" + consumerMaxPollRecords + "&amp;amp;seekTo=" + "beginning" + "&amp;amp;groupId=" + "process") .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader("Authorization",constant("Basic XXXXXXX")) .setHeader("Content-Type",constant("application/json")) .to("https://elasticsearch-sample-es-http:9200/process/process") .log("${body}"); from("kafka:" + "rhpam-tasks" + "?brokers=" + kafkaBootstrap + "&amp;amp;maxPollRecords=" + consumerMaxPollRecords + "&amp;amp;seekTo=" + "beginning" + "&amp;amp;groupId=" + "task") .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader("Authorization",constant("Basic XXXXXX")) .setHeader("Content-Type",constant("application/json")) .to("https://elasticsearch-sample-es-http:9200/tasks/tasks") .log("${body}"); We can now start visualizing the process/task metrics on Kibana. With this we have a whole set up within OpenShift for event-driven business applications that are focused on business process automation. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FLvqxonrVD8" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html</feedburner:origLink></entry><entry><title>Using virtual functions with DPDK on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Mp6zmi4tA4Y/using-virtual-functions-dpdk-red-hat-openshift" /><author><name>Wuxin Zeng</name></author><id>488c7bf9-6821-48b1-a11f-47b59d539f4b</id><updated>2021-08-27T07:00:00Z</updated><published>2021-08-27T07:00:00Z</published><summary type="html">&lt;p&gt;For many years, organizations have optimized their networking hardware by running multiple functions and containers on &lt;a href="https://docs.openshift.com/container-platform/4.3/networking/hardware_networks/about-sriov.html"&gt;Single Root I/O Virtualization&lt;/a&gt; (SR-IOV) network devices. The SR-IOV specification assigns a portion of a network interface card (NIC) or another device to a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; pod, so that you can share the same physical NIC among multiple pods while giving the pods direct access to the network. Organizations also use the &lt;a href="https://www.dpdk.org/"&gt;Data Plane Development Kit&lt;/a&gt; (DPDK) to accelerate network traffic. This article shows you how to set up SR-IOV and DPDK on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and run virtual functions in that environment.&lt;/p&gt; &lt;h2&gt;Configuring SR-IOV on OpenShift&lt;/h2&gt; &lt;p&gt;You can set up SR-IOV either by editing configuration files or by using the OpenShift web console, as shown in the sections that follow. In either case, you have to create a namespace, an OperatorGroup, and a Subscription.&lt;/p&gt; &lt;h3&gt;Editing the configuration files&lt;/h3&gt; &lt;p&gt;First, create a namespace for the SR-IOV Operator. In this example, we give our namespace the name &lt;code&gt;openshift-sriov-network-operator&lt;/code&gt; and assign it a run level of 1 through the &lt;code&gt;openshift.io/run-level&lt;/code&gt; label:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: openshift-sriov-network-operator labels: openshift.io/run-level: "1" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create the OperatorGroup and bind it to the namespace we've just created:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: sriov-network-operators namespace: openshift-sriov-network-operator spec: targetNamespaces: - openshift-sriov-network-operator &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create the Subscription on the SR-IOV Operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/vialpha1 kind: Subscription metadata: name: openshift-sriov-network-operator-subscription namespace: openshift-sriov-network-operator spec: channel: "4.4" name: sriov-network-operator source: redhat-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Using the web console&lt;/h3&gt; &lt;p&gt;Instead of editing configuration files directly, you can do the configuration through the OpenShift console.&lt;/p&gt; &lt;p&gt;The demonstration in Figure 1 shows how to create a namespace object. If you use the &lt;strong&gt;Create Project&lt;/strong&gt; button to create the namespace, you will not be able to name it &lt;code&gt;openshift-sriov-network-operator&lt;/code&gt; because OpenShift does not allow you to create projects with names starting with &lt;code&gt;openshift-&lt;/code&gt;. You can work around the limitation by creating a namespace object.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/createNamespace.gif" width="1336" height="848" alt="How to use the OpenShift web console to create a namespace object." typeof="Image" /&gt;&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Creating a namespace object using the OpenShift web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then, as shown in Figure 2, you can go to the OperatorHub to install the SR-IOV Network Operator, which creates both the OperatorGroup and the Subscription.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/OperatorGroup.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/OperatorGroup.gif" width="1215" height="784" alt="How to use the OpenShift web console to create the OperatorGroup and Subscription." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Creating the OperatorGroup and Subscription using the web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The Node Feature Discovery Operator&lt;/h2&gt; &lt;p&gt;The SR-IOV Network Node Policy requires the following node selector in the configuration file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;feature.node.kubernetes.io/network-sriov.capable: "true"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can install the Node Feature Discovery (&lt;code&gt;nfd&lt;/code&gt;) Operator to automatically label nodes so that you don’t need to add them manually. The Node Feature Discovery Operator allows you to find and label resources in all nodes and to include the node selector that will be required in configuring the network node policy. This operator finds nodes that are ready to support workloads with SR-IOV ports. Use the following YAML to install the operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-operators spec: channel: "4.4" name: nfd source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, use the Node Feature Discovery Operator to create a custom resource definition (CRD) that will be managed by the operator. An example of the node feature discovery CRD follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: nfd.openshift.io/v1alpha1 kind: NodeFeatureDiscovery metadata: name: nfd-master-server namespace: openshift-operators spec: namespace: openshift-nfd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check that the operator is working by reviewing the node labels as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get node --show-labels &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use the console to validate the node labels, as shown in Figure 3. In the console, select &lt;strong&gt;Compute—&gt;Nodes&lt;/strong&gt;, select the node, and scroll down to see the list of node labels.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/nodelabels.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/nodelabels.gif" width="1245" height="789" alt="How to use the OpenShift web console to validate node labels." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Using the OpenShift web console to validate node labels.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Configuring NICs and virtual functions&lt;/h2&gt; &lt;p&gt;The next configuration task is to configure the NICs that will be allocable to provide SR-IOV ports. You need to specify the physical NIC as well as the number of virtual functions that can be used per NIC.&lt;/p&gt; &lt;p&gt;Not all NICs have the same number of virtual functions, so you need first to check the maximum number of virtual functions for each of your NICs. Use the Network Node State CRD in the SR-IOV Network Operator to provide the information. Enter the following command to run the operator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get sriovnetworknodestate -n openshift-sriov-network-operator &lt;node name&gt; -o yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Only the SR-IOV capable NICs have virtual functions. Other NICs will show none.&lt;/p&gt; &lt;p&gt;Alternatively, you can review the SR-IOV capable NICs in the web console, as shown in Figure 4. Go to &lt;strong&gt;Installed Operators&lt;/strong&gt;, select the &lt;strong&gt;Sriov&lt;/strong&gt; network state, click on a &lt;strong&gt;Sriov&lt;/strong&gt; network node, and click on &lt;strong&gt;YAML&lt;/strong&gt; to see the details.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/viewNICs.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/viewNICs.gif" width="1223" height="789" alt="How to review the SR-IOV capable NICs in the OpenShift web console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Reviewing SR-IOV capable NICs in the OpenShift web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now select the SR-IOV capable NICs that you want to use in the environment. For this selection, you can use the SRIOV network node policy custom resource definition created by the SR-IOV Network Operator. You can include the name of the NIC physical function and the range of virtual functions to be used in the &lt;code&gt;nicSelector&lt;/code&gt; specification, in the following format:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;&lt;pfname&gt;#&lt;first_vf&gt;-&lt;last_vf&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can change the driver type for the virtual functions in the &lt;code&gt;deviceType&lt;/code&gt; specification, which allows a choice of &lt;code&gt;netdevice&lt;/code&gt; or &lt;code&gt;vfio-pci&lt;/code&gt; for each virtual function. The &lt;code&gt;netdevice&lt;/code&gt; option performs the device binding in kernel space, whereas &lt;code&gt;vfio-pci&lt;/code&gt; does the binding in user space. The default value is &lt;code&gt;netdevice&lt;/code&gt;. However, because we’re using DPDK in this example, we will be working in user space, so we'll select &lt;code&gt;vfio-pci&lt;/code&gt; for the device type to perform the binding in user space.&lt;/p&gt; &lt;p&gt;Figure 5 shows how to select the device type using the console. Select the SR-IOV Network Operator, and create an SR-IOV network node policy instance.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/selectDeviceType.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/selectDeviceType.gif" width="1245" height="789" alt="How to use the OpenShift web console to select the device type." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Selecting the device type in the web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The following YAML file shows that the NIC selection is done per the SR-IOV network node policy:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-pcie namespace: openshift-sriov-network-operator spec: resourceName: pcieSRIOV nodeSelector: feature.node.kubernetes.io/network-sriov.capable: "true" mtu: 1500 numVfs: 64 nicSelector: pfNames: ["ens1f0#0-49", "ens1f1#0-49"] deviceType: netdevice isRdma: false&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring the network&lt;/h2&gt; &lt;p&gt;Next, you need to configure the network that will be attached to your SR-IOV ports in order to have SR-IOV working in your environment. When you configure the network, you can customize some aspects. If you don’t have a DHCP server in your network, you can define static IP addresses on your pods and enable static IP capability. Also, make sure the namespace is the one where you’ll be using the SR-IOV network:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: sriovnet1 namespace: openshift-sriov-network-operator spec: ipam: | { "type": "static", "addresses": [ { "address": "192.168.99.0/24", "gateway": "192.168.99.1" } ], "dns": { "nameservers": ["8.8.8.8"], "domain": "test.lablocal", "search": ["test.lablocal"] } } vlan: 0 spoofChk: 'on' trust: 'off' resourceName: onboardSRIOV networkNamespace: test-epa capabilities: '{"ips": true}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these configuration changes, you should have SR-IOV working in your environment. To test your configuration, create two pods and add a secondary network using the SR-IOV network. You will also need to configure static IP addresses in both pods. Add &lt;code&gt;nodeName&lt;/code&gt; definitions to force the pods to run in different worker nodes. Once the pods are running, just ping one pod from the other.&lt;/p&gt; &lt;h2&gt;Configuring DPDK on OpenShift&lt;/h2&gt; &lt;p&gt;Configuring DPDK follows the same steps for configuring SR-IOV, with a few modifications. First, DPDK requires configuring huge pages along with the SR-IOV configuration. And, as mentioned earlier, when using DPDK, you need to select a device type of &lt;code&gt;vfio-pci&lt;/code&gt; to bind the device in user space.&lt;/p&gt; &lt;p&gt;An example for an Intel NIC follows:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you’re using a Mellanox NIC, you must use the &lt;code&gt;netDevice&lt;/code&gt; driver type and set &lt;code&gt;isRdma&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-onboard-dpdk namespace: openshift-sriov-network-operator spec: resourceName: onboardDPDK nodeSelector: feature.node.kubernetes.io/network-sriov.capable: "true" mtu: 1500 numVfs: 64 nicSelector: pfNames: ["eno5#50-59", "eno6#50-59"] isRdma: false deviceType: vfio-pci&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The SR-IOV network object configuration for DPDK is the same as the one used for a "plain" SR-IOV configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: dpdknet1 namespace: openshift-sriov-network-operator spec: ipam: | { "type": "static", "addresses": [ { "address": "192.168.155.0/24", "gateway": "192.168.155.1" } ], "dns": { "nameservers": ["8.8.8.8"], "domain": "testdpdk.lablocal", "search": ["testdpdk.lablocal"] } } vlan: 0 spoofChk: 'on' trust: 'off' resourceName: onboardDPDK networkNamespace: test-epa capabilities: '{"ips": true}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lastly, to test DPDK, use an image that uses DPDK to create a pod that includes both the huge pages configuration and the SR-IOV network definition.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I have tried to show that the popular and powerful SR-IOV and DPDK capabilities of network devices are easy to set up on OpenShift. The web console simplifies many tasks, and using operators automates a lot of the configuration. Try these features on your own projects to save resources and increase the capabilities of your network hardware.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/27/using-virtual-functions-dpdk-red-hat-openshift" title="Using virtual functions with DPDK on Red Hat OpenShift"&gt;Using virtual functions with DPDK on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Mp6zmi4tA4Y" height="1" width="1" alt=""/&gt;</summary><dc:creator>Wuxin Zeng</dc:creator><dc:date>2021-08-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/27/using-virtual-functions-dpdk-red-hat-openshift</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 27 August 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/J-DgehC05Y8/weekly-2021-08-27.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="vert.x" /><category term="java" /><category term="narayana" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-08-27.html</id><updated>2021-08-27T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, vert.x, java, narayana"&gt; &lt;h1&gt;This Week in JBoss - 27 August 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_calling_all_kogito_dmn_contributors"&gt;Calling all Kogito DMN contributors&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Guilherme Carreiro brings us the second in a series of posts that guide you through contributing to Kogito’s DMN editor, &lt;a href="https://blog.kie.org/2021/08/dmn-editor-contributors-guide-part-2.html"&gt;DMN editor – Contributors Guide – Part 2&lt;/a&gt;. The post gives a nice overview of the DMN and the BPMN editors then links to some additional resources that guide you through the tasks of creating and testing Reactive components.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_resteasy_reactive_in_quarkus_2_2"&gt;RESTEasy Reactive in Quarkus 2.2&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Clément gives a very insightful look at how Quarkus and RESTEasy Reactive make your life easier when implementing HTTP APIs. He also explains different dispatching strategies in Quarkus 2.2, based on method signatures to intelligently decide whether methods should be called on the I/O thread or a worker thread at build time.&lt;/p&gt; &lt;p&gt;Go read the full details in Clément’s post, &lt;a href="https://quarkus.io/blog/resteasy-reactive-smart-dispatch/"&gt;RESTEasy Reactive - To block or not to block&lt;/a&gt; and take a look at some of the code examples he provides.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_narayana_lra_annotation_checker"&gt;Narayana LRA annotation checker&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The Narayana team have provided a Maven plugin that checks LRA (Long Running Actions) annotations. This plugin helps developers avoid errors and follow the rules of the LRA specification. There’s also a sample Quarkus project in that shows you how to use it.&lt;/p&gt; &lt;p&gt;You can read Ondra Chaloupka’s article here: &lt;a href="https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html"&gt;LRA annotation checker Maven plugin&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_devconf_us_2021_designing_your_best_architecture_diagrams_workshop"&gt;DevConf.US 2021 - Designing your best architecture diagrams workshop&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Over on Eric Schabell’s blog, he announces the &lt;a href="https://www.schabell.org/2021/07/devconfus-2021-designing-your-best-architecture-diagrams-workshop.html"&gt;Designing your best architecture diagrams&lt;/a&gt; workshop that he will deliver at this year’s DevConf.US on Thursday, September 2. Eric’s workshop takes attendees through the process of using open-source tooling to design architecture diagrams like an expert.&lt;/p&gt; &lt;p&gt;Congrats to Eric on getting the workshop accepted. I’m sure it will be an invaluable workshop for all those who face the challenges of communicating complex architectural designs to project teams.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_apache_camel_at_kubecon_eu"&gt;Apache Camel at Kubecon EU&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Nicola Ferraro presented Apache Camel at the Kubecon EU conference that took place on August 19th, 2020.&lt;/p&gt; &lt;p&gt;Nicola’s presentation focused on Camel K integration with Knative with a detailed explanation and a super cool demo. Visit his blog to read details and watch the video at &lt;a href="https://www.nicolaferraro.me/2020/09/08/serverless-integration-kubecon/"&gt;Serverless Integration on Kubernetes with Apache Camel K&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Let’s start things off this week with a look the latest releases.&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vertx-4-2-Beta1-released/"&gt;Eclipse Vert.x 4.2.0.Beta1&lt;/a&gt;! Great to see the first beta is here!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-1-4-final-released/"&gt;Quarkus 2.1.4.Final&lt;/a&gt; is out! Follow the link to find the 2.1 migration guide and complete list of changes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/08/kogito-1-10-0-released.html"&gt;Kogito 1.10.0&lt;/a&gt; is released alongside &lt;a href="https://blog.kie.org/2021/08/kogito-tooling-0-12-0-released.html"&gt;Kogito Tooling 0.12.0&lt;/a&gt; and the &lt;a href="https://github.com/kiegroup/kogito-operator/releases/tag/v1.10.0"&gt;Kogito Operator and CLI Version 1.10.0&lt;/a&gt;. Congrats to the entire Kogito team on all the hard work!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/08/keycloak-1502-released"&gt;Keycloak 15.0.2&lt;/a&gt; is here. Grab the download link and docs from the link.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/J-DgehC05Y8" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-08-27.html</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 5: Building good containers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BAjbWYmGu_w/introduction-nodejs-reference-architecture-part-5-building-good-containers" /><author><name>Michael Dawson</name></author><id>d329e613-6428-42d6-93de-40df642014a6</id><updated>2021-08-26T07:00:00Z</updated><published>2021-08-26T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Containers&lt;/a&gt; are often the unit of deployment in modern applications. An application is built into one or more container images using Docker or Podman, and then those images are deployed into production.&lt;/p&gt; &lt;p&gt;A container packages code (in our case, written in &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;) along with its dependencies so that they can be easily deployed as a unit. The &lt;a href="https://opencontainers.org/"&gt;Open Container Initiative&lt;/a&gt; (OCI) defines the standard for what makes up a container.&lt;/p&gt; &lt;p&gt;This article dives into the discussions that went into creating the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md"&gt;Building Good Containers&lt;/a&gt; section of the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt;. That section focuses on how the container is built, versus how to structure an application for deployment in a container. Other sections in the reference architecture, like &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/healthchecks.md"&gt;Health Checks&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;Logging&lt;/a&gt;, cover how to structure an application for cloud-native deployments.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;strong&gt;Part 5&lt;/strong&gt;: Building good containers&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;What makes a good production container?&lt;/h2&gt; &lt;p&gt;Before we dive into the recommendations for building good containers, what do we mean by a "good" container in the first place? What this means to the Red Hat and IBM team members is that the container:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Applies best practices for security.&lt;/li&gt; &lt;li&gt;Is a reasonable size.&lt;/li&gt; &lt;li&gt;Avoids common pitfalls with running a process in a container.&lt;/li&gt; &lt;li&gt;Can take advantage of the resources provided to it.&lt;/li&gt; &lt;li&gt;Includes what’s needed to debug production issues when they occur.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While the relative priority between these can differ across teams, these were generally important based on our experience.&lt;/p&gt; &lt;h2&gt;What base images to start with?&lt;/h2&gt; &lt;p&gt;In most cases, teams build their containers based on a pre-existing image that includes at least the operating system (OS) and commonly also includes the runtime—in our case, Node.js. In order to build good containers, it is important to start on solid footing by choosing a base container that is well maintained, is scanned and updated when vulnerabilities are reported, keeps up with new versions of the runtime, and (if required by your organization) has commercial support. The reference architecture includes two sections that talk about containers: &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/nodejs-versions-images.md#container-images"&gt;Container images&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/nodejs-versions-images.md#commercially-supported-containers"&gt;Commercially Supported Containers&lt;/a&gt;. Most of the teams within Red Hat and IBM are already using or moving toward using the Node.js &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images &lt;/a&gt; (UBI) for Node.js deployments.&lt;/p&gt; &lt;h2&gt;Apply security best practices&lt;/h2&gt; &lt;p&gt;The first thing we talked about with respect to building good containers is making sure we applied security best practices. The two recommendations that came from these discussions were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Build containers so that your application runs as non-root.&lt;/li&gt; &lt;li&gt;Avoid reserved (privileged) ports (1–1023) inside the container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The reason for building containers so that your application runs as non-root is well-documented, and we found it was a common practice across the team members. For a good article that dives into the details, see &lt;a href="https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b#:~:text=Containers%20are%20not%20trust%20boundaries,a%20container%20on%20your%20server"&gt;Processes In Containers Should Not Run As Root&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why should you avoid using reserved (privileged) ports (1-1023)? Docker or &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; will just map the port to something different anyway, right? The problem is that applications not running as root normally cannot bind to ports 1-1023, and while it might be possible to allow this when the container is started, you generally want to avoid it. In addition, the Node.js runtime has some limitations that mean if you add the privileges needed to run on those ports when starting the container, you can no longer do things like set additional certificates in the environment. Since the ports will be mapped anyway, there is no good reason to use a reserved (privileged) port. Avoiding them can save you trouble in the future.&lt;/p&gt; &lt;h3&gt;A real-world example: A complicated migration&lt;/h3&gt; &lt;p&gt;Using reserved (privileged) ports inside a container led to a complicated migration process for one of our teams when they later wanted to move to a new base container that was designed to run applications as non-root.&lt;/p&gt; &lt;p&gt;The team had many &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; all using the same set of internal ports, and they wanted to be able to slowly update and deploy individual microservices without having to modify the configurations outside of the container. Using different ports internally would have meant they would have to maintain the knowledge of which microservices used which ports internally, and that would make the configuration more complex and harder to maintain. The problem was that with the new base image, the microservices could no longer bind to the internal privileged port they had been using before.&lt;/p&gt; &lt;p&gt;The team thought, "Okay, so let's just use iptables or some other way to redirect so that even when the application binds to a port above 1023, Kubernetes still sees the service as exposed on the original privileged port." Unfortunately, that's not something that developers are expected to do in containers, and base containers don't include the components for port forwarding!&lt;/p&gt; &lt;p&gt;Next, they said, "Okay, let's give the containers the privileges required so that a non-root user can connect to the privileged port." Unfortunately, due to the issue in Node.js, that led to not being able to set additional certificates that they needed. In the end, the team found a way to migrate, but it was a lot more complicated than if they had not been using privileged ports.&lt;/p&gt; &lt;h2&gt;Keep containers to a reasonable size&lt;/h2&gt; &lt;p&gt;A common question is, "Why does container size matter?" The expectation is that with good layering and caching, the total size of a container won't end up being an issue. While that can often be true, environments like Kubernetes make it easy for containers to spin up and down and do so on different machines. Each time this happens on a new machine, you end up having to pull down all of the components. The same happens for new deployments if you updated all of the layers starting at the OS (maybe to address CVEs).&lt;/p&gt; &lt;p&gt;The net is that while we've not seen complaints or had problems in our deployments with respect to the size on disk, the compressed size that might need to be transferred to a machine has led our teams to strive to minimize container size.&lt;/p&gt; &lt;p&gt;A common practice we discussed was multi-stage builds, where you build in a larger base container and then copy the application artifacts to a smaller deployment image. The document &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;Use multi-stage builds&lt;/a&gt; provides a good overview of how to do that.&lt;/p&gt; &lt;h2&gt;Support efficient iterative development&lt;/h2&gt; &lt;p&gt;The discussions on keeping container sizes reasonable also resulted in a few additional recommendations from our experience that I was unaware of before. (The process of putting together the reference architecture has been a great learning experience all around.)&lt;/p&gt; &lt;p&gt;The first was to use the &lt;code&gt;.dockerignore&lt;/code&gt; file. Once I thought about it, it made a lot of sense, as I'd run into one of the issues it addresses a number of times. If you test locally and do an &lt;code&gt;npm install&lt;/code&gt;, you end up with the &lt;code&gt;node_modules&lt;/code&gt; directory locally. When you run your Docker file, it will take longer, as it copies that directory over even though it won't necessarily be used in the build step (and if it is, that could mess things up). Assuming you are using a multi-stage build, it won't affect your final image size, but it does affect the speed of development as you iterate.&lt;/p&gt; &lt;p&gt;The second recommendation was to use a dependency image. For many applications, the build time is dominated by the time it takes to build the dependencies. If you break out your pipeline so that you build a dependency image and then layer your application into that image, the process of updating and testing the application can be much faster. This is because, for most of the iterations, you will not have updated the dependencies and can skip the slower rebuild of the dependency layer.&lt;/p&gt; &lt;h2&gt;Build containers that can take advantage of the resources provided&lt;/h2&gt; &lt;p&gt;The nice thing about using containers is that it decouples the application, microservice, etc., from the physical resources on which it will be deployed. It also means that the resources available to the container might change. Kubernetes, Docker, and Podman all provide ways to change the available resources when a container is started. If you don't plan or think about this in advance, you can end up with a container that overuses or underuses the resources available to it, resulting in poorer performance than expected.&lt;/p&gt; &lt;p&gt;In our discussions, we found that teams had developed patterns to start Node.js applications within containers such that they could leverage the amount of memory made available when the container was deployed. The &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#setting-memory-limits"&gt;reference architecture shares this pattern&lt;/a&gt; as good practice so that your application leverages the available amount of resources. Since Node.js is "approximately" single-threaded, we had not found the need to pass through available CPU resources to the same extent.&lt;/p&gt; &lt;h2&gt;Be ready to debug production issues when they occur&lt;/h2&gt; &lt;p&gt;When things go wrong in production, you often need additional tools to help investigate what is going on. While we did not have a common set of tools to recommend from across our teams at this point, there was consensus that it is best practice to include key tools that you might need for problem investigation. This is one reason why we've been working in the Node.js project to pull some diagnostic tools into core (such as &lt;code&gt;node-report&lt;/code&gt;, the ability to generate heap dumps, and the sampling heap profiler).&lt;/p&gt; &lt;h2&gt;Avoid common pitfalls when running a process in a container&lt;/h2&gt; &lt;p&gt;Running a Node.js process in a container is different from running on a full operating system. This results in a couple of common pitfalls related to signals, child processes, and zombies, in no particular order. Our teams ran into a number of these challenges, which resulted in the recommendations to &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#use-a-process-manager"&gt;use a process manager&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#avoiding-using-npm-to-start-application"&gt;avoid the use of &lt;code&gt;npm start&lt;/code&gt;&lt;/a&gt;. There is not much to add here (the reference architecture provides helpful resources for further reading), other than to say these are real-world issues that one or more of our teams have run into.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Building good containers can result in both faster development cycles and better deployments with fewer problems. In this article, we've shared some of the discussion and background that resulted in the recommendations in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md"&gt;Building Good Containers&lt;/a&gt; section of the Node.js reference architecture.&lt;/p&gt; &lt;p&gt;We hope you find these recommendations useful. While you wait for the next installment in the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Introduction to the Node.js reference architecture series&lt;/a&gt;, you can check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, you can also explore the &lt;a href="m/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers" title="Introduction to the Node.js reference architecture, Part 5: Building good containers"&gt;Introduction to the Node.js reference architecture, Part 5: Building good containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BAjbWYmGu_w" height="1" width="1" alt=""/&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2021-08-26T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers</feedburner:origLink></entry><entry><title type="html">Quarkus 2.1.4.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ofQfKz4Xbos/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-1-4-final-released/</id><updated>2021-08-26T00:00:00Z</updated><content type="html">We just released Quarkus 2.1.4.Final, our fourth (and probably last!) maintenance release on top of 2.1. It is a safe upgrade for anyone already using 2.1. If you are not using 2.1 already, please refer to the 2.1 migration guide. Full changelog You can get the full changelog of 2.1.4.Final...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ofQfKz4Xbos" height="1" width="1" alt=""/&gt;</content><dc:creator>Guillaume Smet</dc:creator><feedburner:origLink>https://quarkus.io/blog/quarkus-2-1-4-final-released/</feedburner:origLink></entry><entry><title>Securing malloc in glibc: Why malloc hooks had to go</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gLP8-dV0DFI/securing-malloc-glibc-why-malloc-hooks-had-go" /><author><name>Siddhesh Poyarekar</name></author><id>0accb512-3e3e-4517-8638-6fdf07c07f2d</id><updated>2021-08-25T07:00:00Z</updated><published>2021-08-25T07:00:00Z</published><summary type="html">&lt;p&gt;Memory access is one of the most basic operations in computer programs. It is also an unending source of program errors in C programs, because memory safety was never really a programming language goal in C. Memory-related issues also comprise a significant part of the &lt;a href="https://cwe.mitre.org/top25/archive/2021/2021_cwe_top25.html"&gt;top 25 security weaknesses&lt;/a&gt; that result in program vulnerabilities.&lt;/p&gt; &lt;p&gt;Memory access also plays an important role in performance, which makes memory management a prime target for performance tuning. It is natural, then, that dynamic memory management in the C runtime should have capabilities that allow fine-grained tracking and customizable actions on allocation events. These features allow users to diagnose memory issues in their programs and if necessary, override the C runtime allocator with their own to improve performance or memory utilization.&lt;/p&gt; &lt;p&gt;This article describes the clash between the quest for flexibility and introspection, on the one hand, and performance and security protections on the other. You'll learn why this clash ultimately led to a major change in how memory allocation (&lt;code&gt;malloc&lt;/code&gt;) is implemented in the &lt;a href="http://www.gnu.org/software/libc/libc.html"&gt;GNU C Library&lt;/a&gt;, or glibc. We'll also discuss how to adapt applications that depended on the old way of doing things, as well as the implications for future versions of Fedora and &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL).&lt;/p&gt; &lt;h2&gt;Debugging malloc in glibc&lt;/h2&gt; &lt;p&gt;Until recently, the GNU C Library, which is the core runtime library for RHEL, provided diagnostic functionality in the form of function pointers that users could overwrite to implement their own allocation functions. These function pointers were collectively called &lt;em&gt;malloc hooks&lt;/em&gt;. If a hook was set to a function address, glibc allocator functions would call the function instead of the internal implementations, allowing programmers to perform arbitrary actions. A programmer could even run a custom function and then, if necessary, call the glibc memory allocator function again (by momentarily setting the hook to &lt;code&gt;NULL&lt;/code&gt;) to get the actual block of memory.&lt;/p&gt; &lt;p&gt;All of the malloc debugging features in glibc (i.e., &lt;code&gt;mtrace&lt;/code&gt;, &lt;code&gt;mcheck&lt;/code&gt;, and the &lt;code&gt;MALLOC_CHECK_&lt;/code&gt; environment variable) were implemented using these hooks. These debugging features, and the hooks in general, were very useful because they provided checking on a more lightweight basis than the memory checking done by full-fledged memory debugging programs such as &lt;a href="https://developers.redhat.com/blog/2021/04/23/valgrind-memcheck-different-ways-to-lose-your-memory"&gt;Valgrind&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2021/05/05/memory-error-checking-in-c-and-c-comparing-sanitizers-and-valgrind"&gt;sanitizers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Malloc hooks in multi-threaded applications&lt;/h2&gt; &lt;p&gt;As applications became increasingly multi-threaded, it was discovered that manipulating malloc hooks in such environments was fraught with risks. All of the debugging features in glibc malloc except &lt;code&gt;MALLOC_CHECK_&lt;/code&gt; were, and continue to be, unsafe in multi-threaded environments. Malloc hooks, the basis of the debugging features, were not the only way to override malloc, either; glibc always supported the &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Replacing-malloc.html#Replacing-malloc"&gt;interposition of malloc functions&lt;/a&gt; by preloading a shared library with those functions. Glibc itself always calls malloc functions through its procedure linkage table (PLT) so that it can invoke the interposed functions.&lt;/p&gt; &lt;p&gt;To make things worse, much of the debugging infrastructure was tightly integrated into system allocator functionality. This made the task of enhancing the allocator unnecessarily complex. Furthermore, there was always the possibility of corner cases inducing unexpected behavior. Finally, implementing debugging features in the system allocator created a minor but unnecessary performance overhead.&lt;/p&gt; &lt;p&gt;The key misfeature of the debugging hooks, though, was their presence as unprotected function pointers that were guaranteed to be executed at specific events. This made the hooks an &lt;a href="https://gist.github.com/romanking98/9aab2804832c0fb46615f025e8ffb0bc"&gt;easy exploit primitive&lt;/a&gt; in practically every program that ran on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; distributions. A trivial search for &lt;a href="https://duckduckgo.com/?q=__malloc_hook+%22house+of%22"&gt;__malloc_hook "house of"&lt;/a&gt; turns up a long list of exploit methods that use the hooks as either an intermediate step or the final goal for the exploit.&lt;/p&gt; &lt;p&gt;Malloc hooks had to go.&lt;/p&gt; &lt;h2&gt;Excising malloc hooks from the main library&lt;/h2&gt; &lt;p&gt;The last of the malloc hook variables were deprecated in glibc 2.32 and new applications were encouraged to use malloc interposition instead. The effect of deprecation was mild: Newer applications just got a warning during the build. In the interest of maintaining backward compatibility, the memory allocator continued to look for hooks and, if available, execute them. In glibc version 2.34 (August 2021), we finally bit the bullet and took support for malloc hooks out of the mainstream library.&lt;/p&gt; &lt;p&gt;The upstream glibc community agreed that malloc debugging features have no place in production. So we moved all debugging features into a separate library named &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; that overrides system malloc behavior to enable debugging. Most importantly, we either moved unprotected hook function pointers into &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; or removed them completely. Doing this eliminated a key exploit primitive from the library.&lt;/p&gt; &lt;h2&gt;Debugging and hardening in a post-hook world&lt;/h2&gt; &lt;p&gt;The new glibc without the problematic hooks will be available in future versions of Fedora and RHEL. With this glibc, malloc debugging features such as &lt;code&gt;MALLOC_CHECK_&lt;/code&gt;, &lt;code&gt;mtrace()&lt;/code&gt;, and &lt;code&gt;mcheck()&lt;/code&gt; will no longer work by default. Users will need to preload &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; to enable these debugging features. Additionally, the &lt;code&gt;__after_morecore_hook&lt;/code&gt;, &lt;code&gt;__default_morecore_hook&lt;/code&gt;, and &lt;code&gt;__morecore&lt;/code&gt; function pointers are no longer read, and the system malloc uses the &lt;code&gt;brk()&lt;/code&gt; and &lt;code&gt;mmap()&lt;/code&gt; system calls to request memory from the kernel.&lt;/p&gt; &lt;p&gt;System administrators may also remove the library from the system and effectively disable malloc debugging and malloc hooks. This is useful hardening for production systems that have strong controls on what files are available on the system.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Separating malloc debugging from the main library is a significant security hardening improvement in glibc. It eliminates an exploit primitive from Linux distributions and adds an opportunity for hardening in both RHEL and Fedora. Simplifying system allocator code also sets the stage for improvements to malloc that may result in better security and performance. Watch out for more interesting changes to the malloc subsystem in future releases of glibc.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/25/securing-malloc-glibc-why-malloc-hooks-had-go" title="Securing malloc in glibc: Why malloc hooks had to go"&gt;Securing malloc in glibc: Why malloc hooks had to go&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gLP8-dV0DFI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2021-08-25T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/25/securing-malloc-glibc-why-malloc-hooks-had-go</feedburner:origLink></entry><entry><title type="html">RESTEasy Reactive - To block or not to block</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/nCCiwUoAHPg/" /><author><name>Clement Escoffier</name></author><id>https://quarkus.io/blog/resteasy-reactive-smart-dispatch/</id><updated>2021-08-25T00:00:00Z</updated><content type="html">In January 2021, the Quarkus team announced RESTEasy Reactive, a novel way to serve HTTP API in Quarkus. Since its introduction, RESTEasy Reactive adoption has been quite good, and we plan to make it the default approach to implement HTTP API shortly. But, wait a minute, what does that mean...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/nCCiwUoAHPg" height="1" width="1" alt=""/&gt;</content><dc:creator>Clement Escoffier</dc:creator><feedburner:origLink>https://quarkus.io/blog/resteasy-reactive-smart-dispatch/</feedburner:origLink></entry><entry><title type="html">DMN editor – Contributors Guide – Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6MHEsAWChgk/dmn-editor-contributors-guide-part-2.html" /><author><name>Guilherme Carreiro</name></author><id>https://blog.kie.org/2021/08/dmn-editor-contributors-guide-part-2.html</id><updated>2021-08-24T11:22:05Z</updated><content type="html">If you’re reading this, probably you’ve already thought about contributing to an open-source project. However, projects generally have some learning curve, which may intimidate newcomers. We do not want that feeling for the DMN editor! So, this is the second of a series of posts that will quickly empower you to contribute to the DMN editor and other Kogito components. Are you excited too? So, this guide is for you! &#x1f603; Today, we’re going to walk through our TypeScript/React components. You’ll learn how to create a new one and modify an existing feature. OVERVIEW Currently, the DMN and the BPMN editors are Java/GWT-based. However, we’re writing new features based on our new stack powered by TypeScript and React. Check how we’re embedding new components into the editors: As we can see in the diagram above, we may have many Components based on React or any other framework. They are entirely uncoupled from the host application (DMN editor) and may be re-used anywhere. Each Editor (DMN or BPMN) has its Loader, which is responsible for grouping all components and teaching how the host application must render each one. Loaders may also include features like compressing, lazy loading, and versioning. CREATING YOUR REACT COMPONENT Okay! But, how can you create those blue boxes? In other words, how can you create new components? Is it difficult? No! You can rely on the yarn new-component command (in the module). It auto-generates all required boilerplates to build a new component, which is not too much, but we created this command to encourage valuable conventions (like component showcases). Check this complete tutorial: . It has all the steps, from creating the component to embedding it in the GWT editor. Now you can create your component by relying on its showcase. So, you don’t need to run the whole editor test, develop, and validate it (if you want to know more about how showcases work, check written by Valentino &#x1f609;). TESTING YOUR COMPONENT IN THE EDITOR At this point, your component is probably running into the DMN editor (if it’s not, you may follow the tutorial above). Still, sometimes we need to connect the dots, integrate everything and see your component in action on GWT by running it in the development mode. For this kind of scenario, you need to wire both worlds. Thus, you’ll be able to perform changes in the GWT or React side and see the changes without restarting the DMN editor web app. Here you may check all steps for wiring your components and the GWT editor: . CHALLENGE TIME! Now you know how React components work in the context of the DMN editor! If you find any problems following the tutorials above, I recorded this video walking through both tutorials: Would you like to start playing with React and real-world challenges? Currently, we have the and the epics with various ideas. If you’d like to start contributing, please reach out the DMN tooling team in the #tooling team on ! &#x1f60a; The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6MHEsAWChgk" height="1" width="1" alt=""/&gt;</content><dc:creator>Guilherme Carreiro</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/dmn-editor-contributors-guide-part-2.html</feedburner:origLink></entry><entry><title>Game telemetry with Kafka Streams and Quarkus, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uX-S72p8EXc/game-telemetry-kafka-streams-and-quarkus-part-1" /><author><name>Evan Shortiss</name></author><id>eef64f30-45cd-43da-915e-2c299dd091c7</id><updated>2021-08-24T07:00:00Z</updated><published>2021-08-24T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; makes it possible to run a variety of analytics on large-scale data. This is the first half of a two-part article that employs one of Kafka's most popular projects, the &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams API&lt;/a&gt;, to analyze data from an online interactive game. Our example uses the Kafka Streams API along with the following Red Hat technologies:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; is a fully hosted and managed Apache Kafka service.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; simplifies provisioning and interaction with managed Kafka clusters, and integration with your applications.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; lets you deploy and test applications quickly.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The two-part article is based on a demonstration from &lt;a href="https://www.redhat.com/en/summit"&gt;Red Hat Summit&lt;/a&gt; 2021. This first part sets up the environment for analytics, and the second part runs the analytics along with replaying games from saved data.&lt;/p&gt; &lt;h2&gt;Why do game data analysis?&lt;/h2&gt; &lt;p&gt;Sources indicate that there are approximately three billion active gamers around the globe. Our pocket-sized supercomputers and their high-speed internet connections have made gaming more accessible than it has ever been. These factors also make data generated during gameplay more accessible and enable developers to constantly improve games after their release. This continuous iteration requires data-driven decision-making that's based on events and telemetry captured during gameplay.&lt;/p&gt; &lt;p&gt;According to &lt;a href="https://steamcharts.com/"&gt;steamcharts.com&lt;/a&gt;, the most popular games often have up to one million concurrent players online—and that’s just the PC versions of those games! That number of players will generate enormous amounts of valuable telemetry data. How can development teams ingest such large volumes of data and put it to good use?&lt;/p&gt; &lt;h2&gt;Uses of game telemetry data&lt;/h2&gt; &lt;p&gt;A practical example can help solidify the value of game telemetry data. The developers of a competitive online FPS (first-person shooter) game could send an enormous amount of telemetry events related to player activity. Some simple events and data might include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The player’s location in the game world, recorded every &lt;em&gt;N&lt;/em&gt; seconds.&lt;/li&gt; &lt;li&gt;Weapon choices and changes.&lt;/li&gt; &lt;li&gt;Each time the player fires a shot.&lt;/li&gt; &lt;li&gt;Each time a player successfully hits an opponent.&lt;/li&gt; &lt;li&gt;Items the player obtains.&lt;/li&gt; &lt;li&gt;Player wins and losses.&lt;/li&gt; &lt;li&gt;Player connection quality and approximate physical location.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Game developers could use the data in these events to determine the busy or hot spots on the game world, the popularity of specific weapons, the players' shooting accuracy, and even the accuracy of players using specific weapons.&lt;/p&gt; &lt;p&gt;Developers could use this information to make game balance adjustments. For example, if a specific weapon is consistently chosen by 50% of players and their accuracy with that weapon is higher than with other weapons, there’s a chance that the weapon is overpowered or bugged.&lt;/p&gt; &lt;p&gt;Network engineers could ingest the player ping time and location to generate Grafana dashboard and alerts, isolate problems, and reference the data when choosing new data center locations.&lt;/p&gt; &lt;p&gt;Marketers could also use the data to market power-ups or incentives to players down on their luck if the game supports microtransactions.&lt;/p&gt; &lt;h2&gt;Example and prerequisites&lt;/h2&gt; &lt;p&gt;This article shows you how to employ Red Hat OpenShift Streams for Apache Kafka with the Kafka Streams API to ingest and analyze real-time events and telemetry reported by a game server, using a practical example. Specifically, you’ll learn how to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Use the Red Hat OpenShift Application Services CLI and OpenShift Streams for Apache Kafka to: &lt;ol&gt;&lt;li&gt;Provision &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka clusters&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Manage Kafka topics.&lt;/li&gt; &lt;li&gt;Connect your OpenShift project to a managed Kafka instance.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Develop a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java application&lt;/a&gt; using the Kafka Streams API to process event data.&lt;/li&gt; &lt;li&gt;Expose HTTP endpoints to the processed data using &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; and MicroProfile.&lt;/li&gt; &lt;li&gt;Deploy &lt;a href="topics/nodejs"&gt;Node.js&lt;/a&gt; and Java applications on Red Hat OpenShift and connect them to your OpenShift Streams for Apache Kafka cluster.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To follow along with the examples, you’ll need:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Access to Red Hat OpenShift Streams for Apache Kafka. You can get a free account at the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Getting started site&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Access to Developer Sandbox. Use the &lt;strong&gt;Get started in the Sandbox&lt;/strong&gt; link on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox welcome page&lt;/a&gt; to get access for free.&lt;/li&gt; &lt;li&gt;The Red Hat OpenShift Application Services command-line interface (CLI). Installation instructions are available &lt;a href="https://github.com/redhat-developer/app-services-guides/tree/main/rhoas-cli#installing-the-rhoas-cli"&gt;on GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The OpenShift CLI. The tool is available at the &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/"&gt;page of files for downloading&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The Git CLI. Downloads are available at the &lt;a href="http://git-scm.com/downloads"&gt;Git download page&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;The demo application: Shipwars&lt;/h2&gt; &lt;p&gt;If you attended Red Hat Summit 2021 you might have already played our &lt;a href="https://arcade.redhat.com/shipwars"&gt;Shipwars game&lt;/a&gt;. This is a browser-based video game (Figure 1) that’s similar to the classic &lt;a href="https://en.wikipedia.org/wiki/Battleship_(game)"&gt;Battleship&lt;/a&gt; tabletop game, but with a smaller 5x5 board and a server-side AI opponent.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_1.png?itok=HdgpQ6FJ" width="1440" height="807" alt="The Shipwars game is a small board of squares shown in a web browser, with ships that the opponent tries to sink by clicking on squares in the board." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A player positioning ships in Shipwars. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Shipwars is a relatively simple game, unlike the complex shooter game described earlier. Despite the simplicity of Shipwars, it generates useful events for you to process. We'll focus on two specific events:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Player: Created when a player connects to the server and is assigned a generated username; for example, "Wool Pegasus" from Figure 1.&lt;/li&gt; &lt;li&gt;Attack: Sent whenever a player attacks. Each player will attack at least 13 times, because a minimum of 14 hits are required to sink all of the opponent’s ships.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You’ll deploy the Shipwars &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and send these events to a managed Kafka instance running on OpenShift Streams for Apache Kafka. You’ll also use Kafka Streams to process events generated by the game. This involves using Kafka Streams to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Join data from two separate Kafka topics.&lt;/li&gt; &lt;li&gt;Create aggregations using the joined data stream.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The join can be used to create a real-time heatmap of player shots. It is also a prerequisite for the aggregation stage.&lt;/p&gt; &lt;p&gt;The aggregations can be used to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create and store records of completed games with each turn in order.&lt;/li&gt; &lt;li&gt;Analyze how human players compare to their AI counterparts.&lt;/li&gt; &lt;li&gt;Continuously update your AI model.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 2 shows the overall architecture of core game services, topics in Red Hat OpenShift Streams for Apache Kafka, and the Kafka Streams topology you’ll create.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_2.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_2.jpeg?itok=bTBl-nml" width="1440" height="953" alt="The architecture of this example contains a game client communicating with a server, which sends data through Red Hat OpenShift Streams for Apache Kafka to Kafka Streams applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Architecture of this example, including the environment for the application and the Kafka Streams topology. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploying the core Shipwars services on the Developer Sandbox&lt;/h2&gt; &lt;p&gt;The README file in Red Hat's &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment"&gt;Shipwars deployment repository&lt;/a&gt; can help you quickly deploy the game.&lt;/p&gt; &lt;p&gt;The first step is to deploy Shipwars on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox&lt;/a&gt;. Shipwars can run without Kafka integration, so you’ll add the Kafka integration after the core game microservices are running.&lt;/p&gt; &lt;p&gt;Get started by accessing the Developer Sandbox. Once you're logged in, you should see two empty projects (namespaces). For example, my projects are &lt;code&gt;eshortis-dev&lt;/code&gt; and &lt;code&gt;eshortis-stage&lt;/code&gt;, as shown in Figure 3. I use the &lt;code&gt;dev&lt;/code&gt; namespace throughout this two-part article.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_3.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_3.jpg?itok=93oILfJb" width="1440" height="878" alt="The OpenShift Developer Sandbox makes two projects available to each user, including a development project." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Projects that are automatically available in Developer Sandbox. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The deployment process for Shipwars uses the OpenShift CLI. Do the following to obtain the OpenShift CLl login command and token:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click your username in the top-right corner of the OpenShift Sandbox UI.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Copy Login Command&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;DevSandbox&lt;/strong&gt; login option when prompted.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Display Token&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Copy and paste the displayed login command into your terminal.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;After a successful login, the OpenShift CLI prints your available projects.&lt;/p&gt; &lt;p&gt;Next, use the &lt;code&gt;git clone&lt;/code&gt; command to clone the &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment"&gt;shipwars-deployment&lt;/a&gt; repository into your workspace. Figure 4 shows both the OpenShift CLI and Git CLI commands.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_4.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_4.jpg?itok=iwrUUf2b" width="1440" height="878" alt="An "oc login" command connects to OpenShift, and a subsequent "git clone" command installs the Shipwars application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Use of the OpenShift CLI login and a "git clone" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment"&gt;script&lt;/a&gt; is included to deploy Shipwars. This is a straightforward script that applies YAML files so you can get to the Kafka content more quickly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/redhat-gamedev/shipwars-deployment $ cd shipwars-deployment/openshift $ NAMESPACE=eshortis-dev ./deploy.game.sh&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;NAMESPACE&lt;/code&gt; variable must be set to a valid namespace in your Developer Sandbox. The &lt;code&gt;eshortis-dev&lt;/code&gt; value used here is an example, and must be replaced with the equivalent for your username, such as &lt;code&gt;myusername-dev&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The script prints the resource types as it creates them. Once it is finished, you can view the deployed services in the OpenShift topology view. You can also click the &lt;strong&gt;Open URL&lt;/strong&gt; link on the NGINX service, as shown in Figure 5, to view and play the Shipwars game against an AI opponent.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_5.jpg?itok=_EvSllFf" width="1440" height="878" alt="The OpenShift Topology View shows that Shipwars consists of servers and a client." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The OpenShift topology view showing the Shipwars core services. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Creating a Kafka instance and topics&lt;/h2&gt; &lt;p&gt;If you’re not familiar with the basics of OpenShift Streams for Apache Kafka, consider reviewing this introductory article: &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. It covers the basics of creating Kafka instances, topics, and service accounts.&lt;/p&gt; &lt;p&gt;In this article, we use the &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; CLI, &lt;code&gt;rhoas&lt;/code&gt;, to manage the creation of everything needed to integrate OpenShift Streams for Apache Kafka with the Shipwars game server you deployed on the Developer Sandbox.&lt;/p&gt; &lt;p&gt;To get started, log in to your &lt;a href="https://cloud.redhat.com/"&gt;cloud.redhat.com&lt;/a&gt; account and create a Kafka instance using the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Login using the browser flow rhoas login # Create a kafka instance rhoas kafka create shipwars&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;kafka create&lt;/code&gt; command will complete within a few seconds, but the Kafka instance won’t be ready at this point. Wait for two or three minutes, then issue a &lt;code&gt;rhoas kafka list&lt;/code&gt; command. This lists your Kafka instances and their status. You can continue with the procedure in this article when the &lt;code&gt;shipwars&lt;/code&gt; instance status is "ready," as shown in Figure 6.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_6.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_6.jpg?itok=WjDLLpA2" width="1440" height="878" alt="The output from a "rhoas kafka list" command shows a managed Kafka instance in OpenShift with a status of "ready."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The managed Kafka instance in OpenShift is ready. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The Red Hat OpenShift Application Services CLI allows you to select a Kafka instance as the context for future commands. Select your newly created &lt;code&gt;shipwars&lt;/code&gt; instance using the &lt;code&gt;rhoas kafka use&lt;/code&gt; command, then create the necessary topics as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Choose the kafka instance to create topics in rhoas kafka use # Create necessary game event topics rhoas kafka topic create shipwars-matches --partitions 3 rhoas kafka topic create shipwars-players --partitions 3 rhoas kafka topic create shipwars-attacks --partitions 3 rhoas kafka topic create shipwars-bonuses --partitions 3 rhoas kafka topic create shipwars-results --partitions 3 # Create topics used by Kafka Streams rhoas kafka topic create shipwars-attacks-lite --partitions 3 rhoas kafka topic create shipwars-streams-shots-aggregate --partitions 3 rhoas kafka topic create shipwars-streams-matches-aggregate --partitions 3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The topic configuration is printed in JSON format after each topic is created, as shown in Figure 7. The only configuration that you’ve explicitly set for your topics is the partition count; other values use sensible defaults.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_7.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_7.jpg?itok=EXhZCFN6" width="1440" height="878" alt="Kafka topics are created in a managed Kafka instance in OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Selecting a managed Kafka instance and creating Kafka topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Connecting to the Shipwars game server&lt;/h2&gt; &lt;p&gt;At this point, you’ve obtained a managed Kafka instance and configured it with the topics required by the Shipwars game. The next step is to configure the Node.js &lt;code&gt;shipwars-game-server&lt;/code&gt; to connect to your topics and send events to them. This is a two-step process.&lt;/p&gt; &lt;p&gt;First, you need to link your OpenShift project to your managed Kafka instance. You can do this by issuing the &lt;code&gt;rhoas cluster connect&lt;/code&gt; command. The command starts a guided process that will ask you to confirm the project and managed Kafka instance being linked, and request you to provide a token obtained from a &lt;a href="https://cloud.redhat.com/openshift/token"&gt;cloud.redhat.com/openshift/token&lt;/a&gt; as shown in Figure 8.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_8.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_8.jpg?itok=BDK7fUA0" width="1440" height="878" alt="An OpenShift token allows a project to be connected to a managed Kafka instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Associating a managed Kafka instance with an OpenShift project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;This process creates the following resources in the target OpenShift project:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A &lt;code&gt;KafkaConnection&lt;/code&gt; custom resource that contains information such as bootstrap server URL and SASL mechanism.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Secret&lt;/code&gt; that contains the service account credentials for connecting to the Kafka instance via SASL SSL.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Secret&lt;/code&gt; that contains your cloud.redhat.com token.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Next, run the &lt;code&gt;rhoas cluster bind&lt;/code&gt; command. This guides you through the process of creating a &lt;code&gt;ServiceBinding&lt;/code&gt; and updates the &lt;code&gt;shipwars-game-server&lt;/code&gt; &lt;code&gt;Deployment&lt;/code&gt; with the credentials required to connect to your managed Kafka instance.&lt;/p&gt; &lt;p&gt;Once the binding process has completed, a new &lt;code&gt;Secret&lt;/code&gt; is generated and mounted into a new pod of the &lt;code&gt;shipwars-game-server&lt;/code&gt; &lt;code&gt;Deployment&lt;/code&gt;. You can explore the contents of this secret using the OpenShift CLI or UI, as shown in Figure 9.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_9.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_9.jpg?itok=tKCyOGh2" width="1440" height="878" alt="A Secrets screen in the OpenShift console shows details about the generated secret that contains managed Kafka connection information." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: A generated Secret that contains managed Kafka connection information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Lastly, you can confirm that the Node.js-based &lt;code&gt;shipwars-game-server&lt;/code&gt; has connected to your managed Kafka instance by viewing its logs. Find the logs by selecting the &lt;code&gt;shipwars-game-server&lt;/code&gt; from the OpenShift topology view, selecting the &lt;strong&gt;Resources&lt;/strong&gt; tab, and clicking the &lt;strong&gt;View logs&lt;/strong&gt; link. The startup logs should contain a message stating that a Kafka producer has connected to the managed Kafka instance bootstrap server URL, as shown in Figure 10.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_1_10.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/openshift_kafka_1_10.jpg?itok=73EP3tJl" width="1440" height="878" alt="The logs from the Shipwars game server show that a Kafka producer has connected to a managed Kafka instance deployed on OpenShift Streams for Apache Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: The Kafka producer has connected to the managed Kafka instance. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion to Part 1&lt;/h2&gt; &lt;p&gt;In this first half of the article, we have set up our game and our environment for analytics. In the second half, we'll run analytics and replay some games. For now, you can verify that game events are being streamed to your managed Kafka instance using a tool such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt;. As an example, the following command outputs the contents of the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic as it receives events from the &lt;code&gt;shipwars-game-server&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kafkacat -t shipwars-attacks-b $KAFKA_BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username=$CLIENT_ID \ -X sasl.password=$CLIENT_SECRET -K " / " -C &lt;/code&gt;&lt;/pre&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/24/game-telemetry-kafka-streams-and-quarkus-part-1" title="Game telemetry with Kafka Streams and Quarkus, Part 1"&gt;Game telemetry with Kafka Streams and Quarkus, Part 1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uX-S72p8EXc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2021-08-24T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/24/game-telemetry-kafka-streams-and-quarkus-part-1</feedburner:origLink></entry></feed>
